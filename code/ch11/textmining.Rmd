---
title: "Text mining - book with Ross & Kevin"
author: "Johnny Zhang"
date: "April 11, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gridExtra)
library(tidyverse)
library(tidytext)
library(sentimentr)
library(syuzhet)
library(lexicon)
library(wordcloud)
library(tm)
library(SnowballC)
library("neuralnet")
library(NeuralNetTools)
library(h2o)
```

# Introduction to text data analysis

## Data

```{r}
profdata <- read.csv("prof1000.original.csv", stringsAsFactors = FALSE)
str(profdata)

#profdata <- profdata[profdata$rating==1 | profdata$rating==2 |  profdata$rating==3 | profdata$rating==4 | profdata$rating==5, ]

#write.csv(profdata, "prof1000.original.csv", row.names=F)
```

## Now plot the data
### Number of categories

```{r}
bar.text <- data.frame(x=1:5, y=100, label=c("Profs get F's too","Mediocre at best","Fine","Solid choice","A real gem"))
bar.rating <- ggplot(data=profdata) +
  geom_bar(aes(rating), alpha=0.5) +
  labs(x="How would you rate this professor as an instructor?", y="Counts") +
  geom_text(data=bar.text, aes(x=x, y=y, label=label),  color='black', angle=90, vjust=0, hjust=0) + 
  theme_grey(base_size = 15)

bar.rating

ggsave("bar-rating.pdf")


bar.text.hard <- data.frame(x=1:5, y=50, label=c("Show up & pass", "Easy A", "The usual", "Makes you work for it", "Hardest thing I've ever done"))

bar.hard <- ggplot(data=profdata) +
  geom_bar(aes(hard), alpha=0.5) +
  labs(x="How hard did you have to work for this class?", y="Counts") +
  geom_text(data=bar.text.hard, aes(x=x, y=y, label=label),  color='black', angle=90, vjust=0, hjust=0)  + 
  theme_grey(base_size = 15)

bar.hard

ggsave("bar-hard.pdf")

## save the two plots in one figure
pdf("bar-rating-hard.pdf", width=14, height=7)
grid.arrange(bar.rating, bar.hard, nrow = 1)
dev.off()

```


## Number of comments each professor

```{r}
n.comments <- table(profdata$profid)
summary(c(n.comments))

pdf('comments-histogram.pdf')
qplot(n.comments, geom='histogram', xlab="Number of comments per professor", ylab="Frequency")
dev.off()

pdf('comments-histogram.pdf', width=7, height=5.5)
hist(n.comments, xlab="Number of comments per professor", ylab="Frequency", main="", breaks=30)
dev.off()
```

## Identify gender information

```{r}
prof.tm <- unnest_tokens(profdata, word, comments)

male.words <- tibble(word=c("he", "him", "his", "he's", "he'd", "he'll", "mr"))
female.words <- tibble(word=c("she", "her", "hers", "she's", "she'd", "she'll", "ms", "mrs", "miss"))

male.info <- inner_join(prof.tm, male.words)
female.info <- inner_join(prof.tm, female.words)

male.info %>% group_by(word) %>% count(word) %>% 
  ungroup() %>% mutate(word = reorder(word, -n))  %>%
  ggplot() +
  geom_bar(aes(x=word, y=n), alpha=0.5, stat = "identity") +
  labs(x="Male word frequency", y="Counts")

ggsave("bar-male-words.pdf")

female.info %>% group_by(word) %>% count(word) %>% 
  ungroup() %>% mutate(word = reorder(word, -n))  %>%
  ggplot() +
  geom_bar(aes(x=word, y=n), alpha=0.5, stat = "identity") +
  labs(x="Female word frequency", y="Counts")

ggsave("bar-female-words.pdf")
```


```{r}
male.info %>% group_by(profid) %>% summarize(n=n()) %>% 
  ggplot() +
  geom_histogram(aes(n)) +
  labs(x="Distribution of number of male words", y="Count")

ggsave("hist-male-words.pdf")

female.info %>% group_by(profid) %>% summarize(n=n()) %>% 
  ggplot() +
  geom_histogram(aes(n)) +
  labs(x="Distribution of number of male words", y="Count")

ggsave("hist-female-words.pdf")
```

### adjust for the number of comments?

```{r}
n.comments <- as.data.frame(n.comments)
names(n.comments) <- c('profid', 'freq')
n.comments$profid <- as.numeric(n.comments$profid)

male.info %>% group_by(profid) %>% summarize(n=n()) %>% 
  inner_join(n.comments) %>% mutate(n=n/freq) %>%
  ggplot() +
  geom_histogram(aes(n)) +
  labs(x="Distribution of number of male words", y="Count")

female.info %>% group_by(profid) %>% summarize(n=n()) %>% 
  inner_join(n.comments) %>% mutate(n=n/freq) %>%
  ggplot() +
  geom_histogram(aes(n)) +
  labs(x="Distribution of number of male words", y="Count")
```

### plot the frequency of gender words

```{r}
male.score <- male.info %>% group_by(profid) %>% summarize(n=n())
female.score <- female.info %>% group_by(profid) %>% summarize(n=n())

gender.info <- full_join(male.score, female.score, by = c("profid")) %>% 
    replace(., is.na(.), 0) %>% rename(total.m = n.x, total.f = n.y)


prof.gender <- gender.info %>% filter(total.m != 0 & total.f !=0)

ggplot(data = prof.gender, aes(x = 1:nrow(prof.gender))) + geom_bar(mapping = aes(y = total.m), 
    stat = "identity", fill = "black") + geom_bar(mapping = aes(y = -total.f), stat = "identity", 
    fill = "black") + xlab("Professors") + ylab("Gender index")+
  geom_text(aes(100, 400, label="Male words"), color="black") +
  geom_text(aes(100, -400, label="Female words"), color="black")

ggsave("bar-male-female-comp.pdf")

```

```{r}
gender.info <- gender.info %>% 
  mutate(gender = ifelse(total.m > total.f, "M", "F")) 
```

### t-test

```{r}
prof.info <- left_join(profdata, gender.info) 

t.test(rating~gender, data = prof.info)

t.test(hard~gender, data = prof.info)

library(effsize)
cohen.d(rating~gender, data = prof.info)

cohen.d(hard~gender, data = prof.info)
```


## Tokenization

```{r}
library(tokenizers)

txt <- "My favorite Professor by far. He is very helpful and willing to help you out the best he can. He will not just give you answers however, but help guide you so that you can learn it on your own. I definately recommend taking him for whatever math courses you can. He requires a lot of work, but it pays off in the run!"

tokenize_sentences(txt) 

tokenize_words(txt)

tokenize_word_stems(txt)

tokenize_ngrams(txt, n = 3, n_min = 3)
```

## matrix representation

```{r}
sentence <- tokenize_sentences(txt)
sentence <- data.frame(sentence = sentence[[1]], id = 1:5, stringsAsFactors = FALSE)

comments <- unnest_tokens(sentence, words, sentence)

word.freq <- comments %>% group_by(id) %>% count(words)

word.dtm <- word.freq %>% cast_dtm(id, words, n)
tm::inspect(word.dtm)

word.subset <- word.dtm[1:5, 1:10]
```

```{r}
prof.tm <- unnest_tokens(profdata, word, comments)

prof.tm %>% count(id, sort = T) %>% 
  ggplot() + 
  geom_histogram(aes(n)) +
  labs(x="Number of words")

ggsave("hist.comment.length.pdf")

word.freq <- prof.tm %>% group_by(id) %>% count(word)

word.dtm <- word.freq %>% cast_dtm(id, word, n)
tm::inspect(word.dtm)
```

```{r}
mat <- read.table(text="Docs but by can far favorite he help it the you
   1   0  1   0   1        1  0    0  0   0   0
   2   0  0   1   0        0  2    1  0   1   1
   3   1  0   1   0        0  1    1  1   0   3
   4   0  0   1   0        0  0    0  0   0   1
   5   1  0   0   0        0  1    0  1   1   0", header=TRUE)

mat <- as.matrix(mat[, -1])
library(Matrix)

mat2 <- Matrix(mat, sparse = TRUE)

```

## Data analysis

### word frequency
```{r}
word.freq <- prof.tm %>% count(word, sort = TRUE)
word.freq %>% 
  top_n(20) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() + 
  labs(y="Number of words") + 
  coord_flip()

ggsave('bar-word-freq.pdf')
```


### stop words

```{r}
stopwords <- read_csv("stopwords.evaluation.csv")
stopwords

prof.tm <- prof.tm %>% anti_join(filter(stopwords, lexicon == "evaluation"))

prof.tm <- prof.tm %>% anti_join(filter(stopwords, lexicon == "optional"))

prof.tm %>% count(word, sort = TRUE) %>% print(n = 20)

word.freq <- prof.tm %>% count(word, sort = TRUE)
word.freq %>% 
  top_n(20) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() + 
  labs(y="Number of words") + 
  coord_flip()

ggsave('bar-word-freq-stop.pdf')
```

### stemming words

```{r}
library(SnowballC)

prof.tm$word <- wordStem(prof.tm$word)

word.freq <- prof.tm %>% count(word, sort = TRUE)
word.freq %>% 
  top_n(20) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() + 
  labs(y="Number of words") + 
  coord_flip()

ggsave('bar-word-freq-stop-stem.pdf')
```

### now remove numbers

```{r}
isNumer <- function(x){
  grepl("[[:digit:]]",x)
}

prof.tm.num <- prof.tm %>%
  filter(!grepl("[[:digit:]]", word))

prof.tm <- prof.tm.num


```

### word cloud

```{r}
library(wordcloud)
word.freq <- prof.tm %>% count(word, sort = TRUE)

pdf('word.cloud1.pdf')
word.freq %>% with(wordcloud(word, n, max.words = 200, 
    random.order = FALSE, rot.per = 0.35,
    colors="black"))
    #colors = brewer.pal(8, "Dark2")))
dev.off()
```

### 2-gram

```{r}
prof.tm <- unnest_tokens(profdata, word, comments, token = "ngrams", n = 2)

prof.tm %>% count(word, sort = TRUE)

prof.separated <- prof.tm %>% separate(word, c("word1", "word2"), sep = " ") %>% 
    anti_join(filter(stopwords, lexicon == "evaluation"), by = c(word1 = "word")) %>% 
    anti_join(filter(stopwords, lexicon == "evaluation"), by = c(word2 = "word"))

prof.count <- prof.separated %>% 
  unite(word, word1, word2, sep = " ") %>% 
  count(word, sort = TRUE)

prof.count %>% separate(word, c("word1", "word2"), sep = " ") %>% 
  filter(word2=="helpful")

## histogram
prof.count %>% 
  top_n(20) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() + 
  xlab(NULL) + 
  coord_flip()

ggsave('bar-word-freq-stop-2gram.pdf')

## word cloud
pdf('word.cloud.2gram.pdf')
prof.count %>% with(wordcloud(word, n, scale = c(2, 0.5), max.words = 100, random.order = FALSE, 
    rot.per = 0.35, colors = "black"))
dev.off()
```

### 3-gram

```{r}
prof.tm <- unnest_tokens(profdata, word, comments, token = "ngrams", n = 3)

prof.separated <- prof.tm %>% separate(word, c("word1", "word2", "word3"), sep = " ") %>% 
    anti_join(filter(stopwords, lexicon == "evaluation"), by = c(word1 = "word")) %>% 
    anti_join(filter(stopwords, lexicon == "evaluation"), by = c(word2 = "word")) %>% 
    anti_join(filter(stopwords, lexicon == "evaluation"), by = c(word3 = "word"))

prof.count <- prof.separated %>% count(word1, word2, word3, sort = TRUE) %>% 
  unite(word, word1, word2, word3, sep = " ")

pdf('word.cloud.3gram.pdf')
prof.count %>% with(wordcloud(word, n, max.words = 100, random.order = FALSE, rot.per = 0.35, 
    colors = "black"))
dev.off()
```


### Network plot

```{r}
library(igraph)

word.network <- prof.separated %>% count(word1, word2, sort = TRUE) %>% filter(n > 
    250) %>% graph_from_data_frame()

library(ggraph)
set.seed(20181006)

a <- arrow(angle = 30, length = unit(0.1, "inches"), ends = "last", type = "open")

test <- ggraph(word.network, layout = "fr") + geom_edge_link(aes(width = n), arrow = a, color='grey') + 
    geom_node_point() + geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  labs(x="", y="")

test
test + xlim(9, 19.5) + scale_fill_continuous(name = "count", trans = "log")
ggsave('word-network.pdf')
```

## Sentiment analysis

```{r}
syuzhet_vector <- get_sentiment(poa_v, method="stanford", path_to_tagger = "C:\\Users\\zzhang4/Downloads/stanford-corenlp-full-2018-10-05")

path_to_stanford_tagger <- "C:/Users/zzhang4/Downloads/stanford-corenlp-full-2018-10-05"

syuzhet_vector <- get_sentiment(s_v, method="stanford", path_to_tagger =path_to_stanford_tagger)

results <- shell(cmd, input = s_v, intern = TRUE, 
        ignore.stderr = TRUE)

get_sentiment(my_example_text, method="bing")

get_sentiment(my_example_text, method="nrc")

get_sentiment(my_example_text, method="afinn")
```

```{r}
library(sentimentr)
library(lexicon)



```
```{r}
hash_sentiment_jockers %>% count(y) %>%
  mutate(y=as.factor(y)) %>%
  ggplot(aes(y, n)) + 
  geom_col() + 
  labs(y="Number of words", x="Score") + 
  coord_flip()

ggsave('bar-syuzhet.pdf')
```


### a sentiment analysis example

```{r}
test <- "My favorite Professor by far. He is very helpful and willing to help you out the best he can. He will not just give you answers however, but help guide you so that you can learn it on your own. I definately recommend taking him for whatever math courses you can. He requires a lot of work, but it pays off in the run!"


test.word <- get_tokens(test)
test.word <- tibble(word=test.word)

test.word %>% inner_join(hash_sentiment_huliu, by=c('word'= 'x'))

get_sentiment(test, method="bing")

test.sent <- get_sentences(test)
get_sentiment(test.sent)
```


### Sentiment analysis of teaching evaluation data

```{r}
prof.sentiment.bing <- get_sentiment(profdata$comments, method="bing")
profdata$sentiment <- prof.sentiment.bing
## histogram
profdata %>%  ggplot(aes(sentiment)) + 
  geom_histogram(color = "black", fill = "white", bins=23)

ggsave('fig-sentiment-bing.pdf')

summary(prof.sentiment.bing)


```

```{r}
## positive words and negative words
prof.comments.sentences <- get_sentences(profdata$comments)
data(hash_sentiment_huliu)
sentiment_words <- extract_sentiment_terms(prof.comments.sentences, polarity_dt = lexicon::hash_sentiment_huliu)

sentiment_counts <- attributes(sentiment_words)$counts

pdf("fig-positive-negative-words.pdf")
par(mfrow = c(1, 2), mar = c(0, 0, 0, 0))
## Positive Words
with(
    sentiment_counts[polarity > 0,],
    wordcloud(words = words, freq = n, min.freq = 1,
          max.words = 100, random.order = FALSE, rot.per = 0.35,
          colors = "black", scale = c(4.5, .75)
    )
)
mtext("Positive Words", side = 3, padj = 10)

## Negative Words
with(
    sentiment_counts[polarity < 0,],
    wordcloud(words = words, freq = n, min.freq = 1,
          max.words = 100, random.order = FALSE, rot.per = 0.35,
          colors = "black", scale = c(4.5, 1)
    )
)
mtext("Negative Words", side = 3, padj = 10)
dev.off()
```


```{r}
## sentiment syuzhet

profdata$sentiment2 <- get_sentiment(profdata$comments, method="syuzhet")

profdata %>%  ggplot(aes(sentiment2)) + 
  geom_histogram(color = "black", fill = "white", bins=23) +
  labs(x='sentiment', y='count')

ggsave('fig-sentiment-syuzhet.pdf')
```

```{r}
## scatterplot
profdata %>% ggplot(aes(x = sentiment, y = sentiment2)) + 
  geom_jitter() +
  labs(x="bing sentiment", y = "syuzhet sentiment")

ggsave('fig-scatter-bing-syuzhet.pdf')

## scatterplot
profdata %>% ggplot(aes(x = rating, y = sentiment2)) + 
  geom_jitter() +
  labs(x="Numerical Rating", y = "syuzhet sentiment")

ggsave('fig-scatter-rating-syuzhet.pdf')
```

### sentiment analysis with shifters

```{r}
vs.rate <- sentiment_attributes(profdata$comments, polarity_dt = lexicon::hash_sentiment_huliu)
vs.rate

sent1 <- sentiment_by(profdata$comments, polarity_dt = lexicon::hash_sentiment_huliu)
sent2 <- sentiment_by(profdata$comments, polarity_dt = lexicon::hash_sentiment_huliu, valence_shifters_dt=NULL)

sentiment4 <- sentiment_by(profdata$comments, polarity_dt = lexicon::hash_sentiment_huliu)

sentiment4 %>%  ggplot(aes(ave_sentiment)) + 
  geom_histogram(color = "black", fill = "white", bins=30) +
  labs(x='sentiment', y='count')

ggsave('fig-sentiment-shifter.pdf')
profdata$sentiment4 <- sentiment4$ave_sentiment

profdata %>% ggplot(aes(x = sentiment, y = sentiment4)) + 
  geom_jitter() +
  labs(x="without shifters", y = "with shifters") +
  geom_abline(intercept = 1, size = 2)

ggsave('fig-scatter-bing-shifter.pdf')



profdata %>% ggplot(aes(x = sent2$ave_sentiment, y = sent1$ave_sentiment)) + 
  geom_jitter() +
  labs(x="without shifters", y = "with shifters") +
  geom_abline(intercept = 0, size = 2)

ggsave('fig-scatter-bing-shifter.pdf')

cor(sent2$ave_sentiment, sent1$ave_sentiment)

sentiment3 <- sentiment_by(profdata$comments)

profdata$sentiment3 <- sentiment3$ave_sentiment

test <- sentiment(profdata$comments, polarity_dt = lexicon::hash_sentiment_huliu)


try <- "Chemistry is not an easy course and it's frustrating, but with him it's actually interesting, and fun."

test <- sentiment(try, polarity_dt = lexicon::hash_sentiment_huliu)
```


#### emotion analysis

```{r}
prof.tm <- unnest_tokens(profdata, word, comments)
nrc <- get_sentiments("nrc")
nrc <- nrc %>% mutate(score = 1)

trust <- prof.tm %>% inner_join(nrc[nrc$sentiment=="trust", c(1,3)]) %>%
  group_by(id) %>%
  summarise(trust=sum(score))

anger <- prof.tm %>% inner_join(nrc[nrc$sentiment=="anger", c(1,3)]) %>%
  group_by(id) %>%
  summarise(anger=sum(score))

anticipation <- prof.tm %>% inner_join(nrc[nrc$sentiment=="anticipation", c(1,3)]) %>%
  group_by(id) %>%
  summarise(anticipation=sum(score))

disgust <- prof.tm %>% inner_join(nrc[nrc$sentiment=="disgust", c(1,3)]) %>%
  group_by(id) %>%
  summarise(disgust=sum(score))

fear <- prof.tm %>% inner_join(nrc[nrc$sentiment=="fear", c(1,3)]) %>%
  group_by(id) %>%
  summarise(fear=sum(score))

joy <- prof.tm %>% inner_join(nrc[nrc$sentiment=="joy", c(1,3)]) %>%
  group_by(id) %>%
  summarise(joy=sum(score))

sadness <- prof.tm %>% inner_join(nrc[nrc$sentiment=="sadness", c(1,3)]) %>%
  group_by(id) %>%
  summarise(sadness=sum(score))

surprise <- prof.tm %>% inner_join(nrc[nrc$sentiment=="surprise", c(1,3)]) %>%
  group_by(id) %>%
  summarise(surprise=sum(score))

emotions <- trust %>% full_join(anger) %>%
  full_join(anticipation) %>% full_join(disgust) %>%
  full_join(fear) %>% full_join(joy) %>%
  full_join(sadness) %>% full_join(surprise)
```

```{r}
## plot the data

  emotions[1:10, ] %>% 
  gather(key='type', value="score", names(emotions)[-1]) %>%
  mutate(type=as.factor(type), id=as.factor(id)) %>%
  ggplot() +
  geom_bar(aes(x=id, y=score, fill = type), position = "stack", stat="identity")+
  labs(x="Comment id", y="Emotion score")

ggsave("fig-barplot-emotion.pdf")


  emotions[1:10, ] %>% 
  gather(key='type', value="score", names(emotions)[-1]) %>%
  mutate(type=as.factor(type), id=as.factor(id)) %>%
  ggplot() +
  geom_bar(aes(x=id, y=score, fill = type), position = "fill", stat="identity")+
  labs(x="Comment id", y="Emotion composition")+
    scale_fill_grey()

ggsave("fig-barplot-emotion-fill.pdf")

```


## Topic modeling - LDA

```{r}
## combine data together

prof.nest <- profdata %>% group_by(profid) %>% 
  summarise(comments = paste(comments, collapse = " "), 
            rating = mean(rating), hard=mean(hard))

prof.tm <- unnest_tokens(prof.nest, word, comments)

stopwords <- read_csv("stopwords.evaluation.csv")
stopwords

prof.tm <- prof.tm %>% anti_join(filter(stopwords, lexicon == "evaluation"))

#prof.tm <- prof.tm %>% anti_join(filter(stopwords, lexicon == "optional"))

prof.tm$word <- wordStem(prof.tm$word)
prof.tm <- prof.tm %>%
  filter(!grepl("[[:digit:]]", word))

prof.dtm <- prof.tm %>%
        count(profid, word) %>%    ## word frequency
        cast_dtm(profid, word, n)  ## convert to dtm matrix

prof.dtm <- removeSparseTerms(prof.dtm, .995)

findFreqTerms(prof.dtm, highfreq=4)


```

### Determine the number of topics

```{r}
k.topics <- 2:9
folding <- rep(1:5, each = 200)
folding <- folding[-1000]

runonce <- function(k, fold) {
    testing.dtm <- which(folding == fold)
    training.dtm <- which(folding != fold)
    
    training.model <- LDA(prof.dtm[training.dtm, ], k = k)
    test.model <- LDA(prof.dtm[testing.dtm, ], model = training.model, control = list(estimate.beta = FALSE))
    
    perplexity(test.model)
}

res <- NULL
library(topicmodels)
for (k in 2:9) {
    for (fold in 1:5) {
        res <- rbind(res, c(k, fold, runonce(k, fold)))
    }
}

```

Plot the results from cross-validation

```{r}
total.perp <- tapply(res[, 3], res[, 1], sum)
round(total.perp)
##    2    3    4    5    6    7    8    9 
## 6305 6021 5955 5901 5847 5825 5838 5811

plot(2:9, total.perp, type = "b", xlab = "Number of topics", ylab = "Perplexity")

qplot(2:9, total.perp, geom=c("point", "line"), xlab = "Number of topics", ylab = "Perplexity")

ggsave("fig-cv-plot.pdf")
```

Now estimate the model with 6 topics

```{r}

prof.lda <- LDA(prof.dtm, k = 6, control=list(seed=20220728))

terms(prof.lda, 10)

prof.topics <- tidy(prof.lda, matrix = "beta")
prof.topics

## terms & topics
prof.terms <- prof.topics %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

prof.terms %>% print(n=60)

reorder_within <- function (x, by, within, fun = mean, sep = "___", ...) 
{
    new_x <- paste(x, within, sep = sep)
    stats::reorder(new_x, by, FUN = fun)
}

## plot the topics and terms
test <- prof.terms %>% 
  mutate(topic=as.factor(topic), term = reorder_within(term, beta, topic, sep="")) %>% 
  ggplot(aes(term, beta, fill = topic)) + 
    geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = "free", labeller = "label_both") + 
    xlab("Terms") + ylab("Topics") + coord_flip() + scale_x_reordered() + scale_fill_grey()
test

ggsave("fig-topics-terms.pdf")
```

Topics for each comment
```{r}
prof.comments <- tidy(prof.lda, matrix = "gamma")
prof.comments

prof.comments %>% group_by(document) %>%
  summarise(total=sum(gamma))

prof.comments %>% arrange(document, topic) %>%
  print(n=30)

topics(prof.lda, 6)[1:6, 1:10]
```

## Neural Network Models

```{r}
profdata <- read.csv("prof1000.original.csv", stringsAsFactors = FALSE)
prof.tm <- unnest_tokens(profdata, word, comments)
head(prof.tm)

stopwords <- read_csv("stopwords.evaluation.csv")
stopwords

prof.tm <- prof.tm %>% anti_join(filter(stopwords, lexicon == "evaluation"))



prof.tm$word <- wordStem(prof.tm$word)
prof.tm <- prof.tm %>%
  filter(!grepl("[[:digit:]]", word))

prof.dtm <- prof.tm %>%
        count(id, word) %>%    ## word frequency
        cast_dtm(id, word, n)  ## convert to dtm matrix

#prof.dtm <- removeSparseTerms(prof.dtm, 1-5/27938)

#findFreqTerms(prof.dtm, highfreq=4)

docs.id <- Docs(prof.dtm)
rating <- hard <- rep(NA, length(docs.id))

for (i in 1:length(docs.id)){
  temp.index <- which(profdata$id == as.numeric(docs.id[i]))
  rating[i] <- profdata$rating[temp.index]
  hard[i] <- profdata$hard[temp.index]
}


prof.word.mat <- as.matrix(prof.dtm)

prof.word.mat <- cbind(rating, prof.word.mat)
prof.word.mat <- as.data.frame(prof.word.mat)

#prof.nn <- neuralnet(rating ~ ., data=prof.word.mat, linear.output = FALSE)

names(prof.word.mat) <- c('rating', paste0("V", 1:(ncol(prof.word.mat)-1)))

prof.model <- paste0(names(prof.word.mat)[-1], collapse="+")
prof.model <- paste("rating ~ ", prof.model)
prof.model <- as.formula(prof.model)

#prof.nn <- neuralnet(prof.model, data=prof.word.mat, linear.output = FALSE)


#prof.pred <- predict(prof.nn, prof.word.mat)

prof.word.mat.cat <- prof.word.mat
prof.word.mat.cat$rating <- as.factor(prof.word.mat$rating)
prof.nn <- neuralnet(prof.model, data=prof.word.mat.cat, linear.output = FALSE)


prof.pred <- predict(prof.nn, prof.word.mat)

save(file="res-all.RData", list=c("prof.pred", "prof.nn"))


prof.pred.cat <- apply(prof.pred, 1, which.max)

## data
prof.word.mat <- as.matrix(prof.dtm)

## data normalization
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

prof.word.mat <- scale(prof.word.mat)

save(file="pro.word.mat.RData", prof.word.mat)

## combine data
prof.net.data <- cbind(rating, prof.word.mat)
prof.net.data <- as.data.frame(prof.net.data)
## variable names
names(prof.net.data) <- c('rating', paste0("V", 1:(ncol(prof.word.mat))))

## categorical variable
prof.word.mat.cat <- prof.net.data
prof.word.mat.cat$rating <- as.factor(prof.net.data$rating)

## model formula
prof.model <- paste0(names(prof.net.data)[-1], collapse="+")
prof.model <- paste("rating ~ ", prof.model)
prof.model <- as.formula(prof.model)

## fit the model
names(prof.word.mat.cat) <- c('rating', paste0("V", 1:(ncol(prof.word.mat))))
prof.nn <- neuralnet(prof.model, data=prof.word.mat.cat, linear.output = FALSE)

## predictation
prof.pred <- predict(prof.nn, prof.word.mat)
prof.pred.cat <- apply(prof.pred, 1, which.max)
cor(prof.pred.cat, rating)

save(file="res-normal-all.RData", list=c("prof-normal.pred", "prof.nn"))

## fit a regression model
prof.lm <- lm(prof.model, data=prof.net.data)

```

